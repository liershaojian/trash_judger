"""
åƒåœ¾åˆ†ç±»æ¨¡å‹å¾®è°ƒè®­ç»ƒè„šæœ¬
æ¨¡å‹: MobileNetV3-Large
æ•°æ®é›†: æ”¯æŒ TrashNet / åä¸ºåƒåœ¾åˆ†ç±» / è‡ªå®šä¹‰æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python training/train.py --data_dir ./data --epochs 20 --batch_size 32
"""

import os
import sys
import json
import argparse
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights

from dataset import WasteDataset, get_data_transforms

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (images, labels) in enumerate(dataloader):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        if (batch_idx + 1) % 10 == 0:
            print(f"  Batch [{batch_idx+1}/{len(dataloader)}] "
                  f"Loss: {loss.item():.4f} "
                  f"Acc: {100.*correct/total:.2f}%")
    
    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc


def validate(model, dataloader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    val_loss = running_loss / len(dataloader)
    val_acc = 100. * correct / total
    return val_loss, val_acc


def main(args):
    print("=" * 60)
    print("ğŸ—‘ï¸  åƒåœ¾åˆ†ç±»æ¨¡å‹å¾®è°ƒè®­ç»ƒ")
    print("=" * 60)
    print(f"æ¨¡å‹: MobileNetV3-Large")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print("=" * 60)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æ•°æ®å¢å¼º
    train_transform, val_transform = get_data_transforms()
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_dir = os.path.join(args.data_dir, 'train')
    val_dir = os.path.join(args.data_dir, 'val')
    
    if not os.path.exists(train_dir):
        print(f"âŒ é”™è¯¯: è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {train_dir}")
        print(f"\nè¯·æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡æ•°æ®:")
        print(f"  {args.data_dir}/")
        print(f"  â”œâ”€â”€ train/")
        print(f"  â”‚   â”œâ”€â”€ cardboard/")
        print(f"  â”‚   â”œâ”€â”€ glass/")
        print(f"  â”‚   â”œâ”€â”€ metal/")
        print(f"  â”‚   â””â”€â”€ ...")
        print(f"  â””â”€â”€ val/")
        print(f"      â””â”€â”€ (åŒä¸Š)")
        return
    
    train_dataset = WasteDataset(train_dir, transform=train_transform)
    val_dataset = WasteDataset(val_dir, transform=val_transform)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=args.num_workers,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    labels = train_dataset.classes
    num_classes = len(labels)
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    print(f"âœ… ç±»åˆ«æ•°: {num_classes}")
    print(f"   ç±»åˆ«: {labels}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)
    
    # å†»ç»“ç‰¹å¾æå–å±‚ï¼ˆå¯é€‰ï¼ŒåŠ å¿«è®­ç»ƒï¼‰
    if args.freeze_backbone:
        print("   å†»ç»“ backbone å±‚")
        for param in model.features.parameters():
            param.requires_grad = False
    
    # ä¿®æ”¹åˆ†ç±»å¤´
    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)
    model = model.to(device)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    best_acc = 0.0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    
    for epoch in range(args.epochs):
        print(f"\n{'='*40}")
        print(f"Epoch [{epoch+1}/{args.epochs}]  LR: {scheduler.get_last_lr()[0]:.6f}")
        print(f"{'='*40}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        
        # éªŒè¯
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"   Train Loss: {train_loss:.4f}  Train Acc: {train_acc:.2f}%")
        print(f"   Val Loss:   {val_loss:.4f}  Val Acc:   {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            save_path = os.path.join(args.output_dir, 'waste_classifier.pt')
            os.makedirs(args.output_dir, exist_ok=True)
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'labels': labels,
                'num_classes': num_classes,
                'model_name': 'MobileNetV3-Large',
                'best_acc': best_acc,
                'epoch': epoch + 1,
                'training_args': vars(args)
            }, save_path)
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.2f}%)")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(args.output_dir, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    
    # åŒæ—¶æ›´æ–° labels.json
    labels_path = os.path.join(args.output_dir, 'labels.json')
    with open(labels_path, 'w', encoding='utf-8') as f:
        json.dump(labels, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {os.path.join(args.output_dir, 'waste_classifier.pt')}")
    print(f"   è®­ç»ƒå†å²: {history_path}")
    print(f"{'='*60}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='åƒåœ¾åˆ†ç±»æ¨¡å‹å¾®è°ƒè®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, default='./data_split',
                        help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./models',
                        help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=20,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-3,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•° (Windows å»ºè®®è®¾ä¸º 0)')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--freeze_backbone', action='store_true',
                        help='æ˜¯å¦å†»ç»“ backbone å±‚')
    
    args = parser.parse_args()
    main(args)
