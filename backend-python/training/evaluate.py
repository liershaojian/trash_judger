"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
è®¡ç®—å‡†ç¡®ç‡ã€æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šç­‰æŒ‡æ ‡
"""

import os
import sys
import json
import argparse
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision.models import mobilenet_v3_large
from sklearn.metrics import (
    confusion_matrix, 
    classification_report, 
    accuracy_score,
    precision_recall_fscore_support
)

from dataset import WasteDataset, get_data_transforms

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_PLOT = True
except ImportError:
    HAS_PLOT = False
    print("æç¤º: å®‰è£… matplotlib å’Œ seaborn å¯ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")


def load_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    labels = checkpoint.get('labels', [])
    num_classes = checkpoint.get('num_classes', len(labels))
    
    # åˆ›å»ºæ¨¡å‹
    model = mobilenet_v3_large(weights=None)
    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)
    
    # åŠ è½½æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    return model, labels


def evaluate(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾"""
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    return np.array(all_preds), np.array(all_labels), np.array(all_probs)


def plot_confusion_matrix(cm, labels, save_path):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    if not HAS_PLOT:
        return
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=labels,
        yticklabels=labels
    )
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")


def plot_training_history(history_path, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    if not HAS_PLOT:
        return
    
    if not os.path.exists(history_path):
        return
    
    with open(history_path, 'r') as f:
        history = json.load(f)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # Loss æ›²çº¿
    axes[0].plot(history['train_loss'], label='Train Loss')
    axes[0].plot(history['val_loss'], label='Val Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Loss æ›²çº¿')
    axes[0].legend()
    axes[0].grid(True)
    
    # Accuracy æ›²çº¿
    axes[1].plot(history['train_acc'], label='Train Acc')
    axes[1].plot(history['val_acc'], label='Val Acc')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].set_title('å‡†ç¡®ç‡æ›²çº¿')
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150)
    plt.close()
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}")


def main(args):
    print("=" * 60)
    print("ğŸ” åƒåœ¾åˆ†ç±»æ¨¡å‹è¯„ä¼°")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è¿è¡Œè®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {args.model_path}")
    model, labels = load_model(args.model_path, device)
    print(f"   ç±»åˆ«æ•°: {len(labels)}")
    print(f"   ç±»åˆ«: {labels}")
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½éªŒè¯æ•°æ®: {args.data_dir}")
    _, val_transform = get_data_transforms()
    val_dataset = WasteDataset(args.data_dir, transform=val_transform)
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False,
        num_workers=4
    )
    print(f"   æ ·æœ¬æ•°: {len(val_dataset)}")
    
    # è¯„ä¼°
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼°...")
    preds, true_labels, probs = evaluate(model, val_loader, device)
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(true_labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, preds, average='weighted'
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ")
    print(f"{'='*60}")
    print(f"å‡†ç¡®ç‡ (Accuracy):  {accuracy*100:.2f}%")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision*100:.2f}%")
    print(f"å¬å›ç‡ (Recall):    {recall*100:.2f}%")
    print(f"F1 åˆ†æ•° (F1-Score): {f1*100:.2f}%")
    
    # åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    print("-" * 60)
    report = classification_report(true_labels, preds, target_names=labels)
    print(report)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, preds)
    print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
    print(cm)
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    report_path = os.path.join(args.output_dir, 'evaluation_report.json')
    report_dict = {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'confusion_matrix': cm.tolist(),
        'labels': labels,
        'classification_report': classification_report(
            true_labels, preds, target_names=labels, output_dict=True
        )
    }
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report_dict, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # ç»˜åˆ¶å›¾è¡¨
    if HAS_PLOT:
        cm_path = os.path.join(args.output_dir, 'confusion_matrix.png')
        plot_confusion_matrix(cm, labels, cm_path)
        
        history_path = os.path.join(os.path.dirname(args.model_path), 'training_history.json')
        if os.path.exists(history_path):
            curve_path = os.path.join(args.output_dir, 'training_curves.png')
            plot_training_history(history_path, curve_path)
    
    print(f"\n{'='*60}")
    print(f"âœ… è¯„ä¼°å®Œæˆ!")
    print(f"{'='*60}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='åƒåœ¾åˆ†ç±»æ¨¡å‹è¯„ä¼°')
    
    parser.add_argument('--model_path', type=str, default='./models/waste_classifier.pt',
                        help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--data_dir', type=str, default='./data/val',
                        help='éªŒè¯é›†ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='./evaluation',
                        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    main(args)
