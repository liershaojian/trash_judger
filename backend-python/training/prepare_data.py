"""
æ•°æ®é›†å‡†å¤‡è„šæœ¬
å¸®åŠ©ä¸‹è½½å’Œç»„ç»‡åƒåœ¾åˆ†ç±»æ•°æ®é›†
"""

import os
import sys
import shutil
import random
from pathlib import Path


def download_trashnet():
    """ä¸‹è½½ TrashNet æ•°æ®é›†"""
    print("=" * 60)
    print("ğŸ“¥ TrashNet æ•°æ®é›†ä¸‹è½½æŒ‡å—")
    print("=" * 60)
    print("""
TrashNet æ˜¯ä¸€ä¸ªç»å…¸çš„åƒåœ¾åˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å« 6 ç±»åƒåœ¾å›¾ç‰‡ã€‚

ä¸‹è½½æ–¹å¼:
---------
1. è®¿é—® GitHub: https://github.com/garythung/trashnet

2. ä¸‹è½½ dataset-resized.zip (å·²è°ƒæ•´å¤§å°çš„ç‰ˆæœ¬ï¼Œçº¦ 100MB)
   ç›´æ¥é“¾æ¥: https://github.com/garythung/trashnet/raw/master/data/dataset-resized.zip

3. è§£å‹åˆ° data ç›®å½•:
   data/
   â”œâ”€â”€ cardboard/
   â”œâ”€â”€ glass/
   â”œâ”€â”€ metal/
   â”œâ”€â”€ paper/
   â”œâ”€â”€ plastic/
   â””â”€â”€ trash/

4. è¿è¡Œæœ¬è„šæœ¬çš„ split å‘½ä»¤åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†:
   python prepare_data.py split --data_dir ./data --split_ratio 0.8

ç±»åˆ«è¯´æ˜:
---------
- cardboard: çº¸æ¿ (403å¼ ) -> å¯å›æ”¶ç‰©
- glass: ç»ç’ƒ (501å¼ ) -> å¯å›æ”¶ç‰©
- metal: é‡‘å± (410å¼ ) -> å¯å›æ”¶ç‰©
- paper: çº¸å¼  (594å¼ ) -> å¯å›æ”¶ç‰©
- plastic: å¡‘æ–™ (482å¼ ) -> å¯å›æ”¶ç‰©
- trash: å…¶ä»–åƒåœ¾ (137å¼ ) -> å¹²åƒåœ¾

æ€»è®¡: 2,527 å¼ å›¾ç‰‡
""")


def download_garbage12():
    """ä¸‹è½½ Garbage Classification 12ç±»æ•°æ®é›†"""
    print("=" * 60)
    print("ğŸ“¥ Garbage Classification (12ç±») æ•°æ®é›†ä¸‹è½½æŒ‡å—")
    print("=" * 60)
    print("""
è¿™æ˜¯ä¸€ä¸ªæ›´å¤§çš„åƒåœ¾åˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«çº¦ 15,000 å¼ å›¾ç‰‡ï¼Œ12 ä¸ªç±»åˆ«ã€‚

ä¸‹è½½æ–¹å¼:
---------
1. è®¿é—® Kaggle: https://www.kaggle.com/datasets/mostafaabla/garbage-classification

2. ä¸‹è½½æ•°æ®é›† (éœ€è¦ Kaggle è´¦å·)

3. è§£å‹åˆ° data ç›®å½•ï¼Œç»“æ„å¦‚ä¸‹:
   data/
   â”œâ”€â”€ battery/
   â”œâ”€â”€ biological/
   â”œâ”€â”€ cardboard/
   â”œâ”€â”€ clothes/
   â”œâ”€â”€ glass/
   â”œâ”€â”€ metal/
   â”œâ”€â”€ paper/
   â”œâ”€â”€ plastic/
   â”œâ”€â”€ shoes/
   â”œâ”€â”€ trash/
   â””â”€â”€ ...

4. è¿è¡Œ split å‘½ä»¤åˆ’åˆ†æ•°æ®é›†

ç±»åˆ«æ˜ å°„åˆ°å››å¤§ç±»:
-----------------
- å¯å›æ”¶ç‰©: cardboard, glass, metal, paper, plastic, clothes, shoes
- æœ‰å®³åƒåœ¾: battery
- å¨ä½™åƒåœ¾: biological (food waste)
- å…¶ä»–åƒåœ¾: trash
""")


def download_huawei():
    """ä¸‹è½½åä¸ºåƒåœ¾åˆ†ç±»æ•°æ®é›†"""
    print("=" * 60)
    print("ğŸ“¥ åä¸ºåƒåœ¾åˆ†ç±»æ•°æ®é›†ä¸‹è½½æŒ‡å—")
    print("=" * 60)
    print("""
åä¸ºäº‘ AI Gallery æä¾›çš„ä¸­å›½æ ‡å‡†å››åˆ†ç±»åƒåœ¾æ•°æ®é›†ã€‚

ä¸‹è½½æ–¹å¼:
---------
1. è®¿é—®åä¸ºäº‘ AI Gallery:
   https://developer.huaweicloud.com/develop/aigallery/dataset/detail?id=xxx
   (æœç´¢ "åƒåœ¾åˆ†ç±»")

2. æˆ–è€…ä½¿ç”¨ ModelArts ç›´æ¥ä¸‹è½½

3. è§£å‹åç»„ç»‡ä¸ºä»¥ä¸‹ç»“æ„:
   data/
   â”œâ”€â”€ train/
   â”‚   â”œâ”€â”€ recyclable/     # å¯å›æ”¶ç‰©
   â”‚   â”œâ”€â”€ hazardous/      # æœ‰å®³åƒåœ¾
   â”‚   â”œâ”€â”€ wet/            # å¨ä½™åƒåœ¾
   â”‚   â””â”€â”€ dry/            # å…¶ä»–åƒåœ¾
   â””â”€â”€ val/
       â””â”€â”€ (åŒä¸Š)

ç‰¹ç‚¹:
-----
- ç¬¦åˆä¸­å›½å›½æ ‡å››åˆ†ç±»
- å›¾ç‰‡æ¥æºäºçœŸå®ç”Ÿæ´»åœºæ™¯
- çº¦ 14,000+ å¼ å›¾ç‰‡
""")


def split_dataset(data_dir, output_dir, split_ratio=0.8):
    """
    å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•ï¼ˆåŒ…å«å„ç±»åˆ«å­ç›®å½•ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        split_ratio: è®­ç»ƒé›†æ¯”ä¾‹
    """
    print(f"\nğŸ“‚ åˆ’åˆ†æ•°æ®é›†: {data_dir}")
    print(f"   è®­ç»ƒé›†æ¯”ä¾‹: {split_ratio}")
    
    data_path = Path(data_dir)
    output_path = Path(output_dir)
    
    # è·å–æ‰€æœ‰ç±»åˆ«
    categories = [d.name for d in data_path.iterdir() 
                  if d.is_dir() and not d.name.startswith('.')]
    
    if not categories:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°ç±»åˆ«ç›®å½•")
        return
    
    print(f"   å‘ç° {len(categories)} ä¸ªç±»åˆ«: {categories}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    train_dir = output_path / 'train'
    val_dir = output_path / 'val'
    
    total_train = 0
    total_val = 0
    
    for category in categories:
        src_dir = data_path / category
        
        # è·å–æ‰€æœ‰å›¾ç‰‡
        images = list(src_dir.glob('*.[jJ][pP][gG]')) + \
                 list(src_dir.glob('*.[pP][nN][gG]')) + \
                 list(src_dir.glob('*.[jJ][pP][eE][gG]'))
        
        if not images:
            print(f"   âš ï¸ {category}: æ— å›¾ç‰‡")
            continue
        
        # éšæœºæ‰“ä¹±
        random.shuffle(images)
        
        # åˆ’åˆ†
        split_idx = int(len(images) * split_ratio)
        train_images = images[:split_idx]
        val_images = images[split_idx:]
        
        # åˆ›å»ºç›®å½•
        train_cat_dir = train_dir / category
        val_cat_dir = val_dir / category
        train_cat_dir.mkdir(parents=True, exist_ok=True)
        val_cat_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        for img in train_images:
            shutil.copy2(img, train_cat_dir / img.name)
        for img in val_images:
            shutil.copy2(img, val_cat_dir / img.name)
        
        total_train += len(train_images)
        total_val += len(val_images)
        print(f"   âœ… {category}: {len(train_images)} train, {len(val_images)} val")
    
    print(f"\nâœ… åˆ’åˆ†å®Œæˆ!")
    print(f"   è®­ç»ƒé›†: {total_train} å¼ ")
    print(f"   éªŒè¯é›†: {total_val} å¼ ")
    print(f"   è¾“å‡ºç›®å½•: {output_path}")


def check_dataset(data_dir):
    """æ£€æŸ¥æ•°æ®é›†ç»“æ„"""
    print(f"\nğŸ“‚ æ£€æŸ¥æ•°æ®é›†: {data_dir}")
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ train/val å­ç›®å½•
    train_dir = data_path / 'train'
    val_dir = data_path / 'val'
    
    if train_dir.exists() and val_dir.exists():
        print("âœ… å·²åˆ’åˆ†ä¸º train/val ç»“æ„")
        dirs_to_check = [('train', train_dir), ('val', val_dir)]
    else:
        print("â„¹ï¸ æœªåˆ’åˆ†ï¼Œæ£€æŸ¥åŸå§‹ç»“æ„")
        dirs_to_check = [('root', data_path)]
    
    for name, dir_path in dirs_to_check:
        print(f"\n[{name}]")
        categories = sorted([d.name for d in dir_path.iterdir() 
                            if d.is_dir() and not d.name.startswith('.')])
        
        total = 0
        for cat in categories:
            cat_dir = dir_path / cat
            images = list(cat_dir.glob('*.[jJ][pP][gG]')) + \
                     list(cat_dir.glob('*.[pP][nN][gG]')) + \
                     list(cat_dir.glob('*.[jJ][pP][eE][gG]'))
            total += len(images)
            print(f"  {cat}: {len(images)} å¼ ")
        
        print(f"  æ€»è®¡: {total} å¼ ")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='åƒåœ¾åˆ†ç±»æ•°æ®é›†å‡†å¤‡å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # download å‘½ä»¤
    download_parser = subparsers.add_parser('download', help='æ˜¾ç¤ºæ•°æ®é›†ä¸‹è½½æŒ‡å—')
    download_parser.add_argument('--dataset', type=str, default='trashnet',
                                 choices=['trashnet', 'garbage12', 'huawei'],
                                 help='æ•°æ®é›†åç§°')
    
    # split å‘½ä»¤
    split_parser = subparsers.add_parser('split', help='åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†')
    split_parser.add_argument('--data_dir', type=str, required=True,
                              help='åŸå§‹æ•°æ®ç›®å½•')
    split_parser.add_argument('--output_dir', type=str, default=None,
                              help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ data_dir ç›¸åŒï¼‰')
    split_parser.add_argument('--split_ratio', type=float, default=0.8,
                              help='è®­ç»ƒé›†æ¯”ä¾‹')
    
    # check å‘½ä»¤
    check_parser = subparsers.add_parser('check', help='æ£€æŸ¥æ•°æ®é›†ç»“æ„')
    check_parser.add_argument('--data_dir', type=str, required=True,
                              help='æ•°æ®ç›®å½•')
    
    args = parser.parse_args()
    
    if args.command == 'download':
        if args.dataset == 'trashnet':
            download_trashnet()
        elif args.dataset == 'garbage12':
            download_garbage12()
        elif args.dataset == 'huawei':
            download_huawei()
    elif args.command == 'split':
        output_dir = args.output_dir or args.data_dir
        split_dataset(args.data_dir, output_dir, args.split_ratio)
    elif args.command == 'check':
        check_dataset(args.data_dir)
    else:
        parser.print_help()
        print("\nç¤ºä¾‹:")
        print("  python prepare_data.py download --dataset trashnet")
        print("  python prepare_data.py split --data_dir ./raw_data --output_dir ./data")
        print("  python prepare_data.py check --data_dir ./data")


if __name__ == '__main__':
    main()
